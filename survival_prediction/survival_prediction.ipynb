{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d8c72b",
   "metadata": {},
   "source": [
    "# Survival Prediction — Titanic dataset\n",
    "\n",
    "This notebook uses the public Titanic dataset (CSV) and demonstrates a reproducible ML workflow:\n",
    "\n",
    "- Data loading and exploratory data analysis (EDA)\n",
    "- Feature engineering and preprocessing\n",
    "- Cross-validated model evaluation and hyperparameter tuning\n",
    "- Visualizations and model interpretability using SHAP and LIME\n",
    "\n",
    "Run the downloader first (if you haven't already):\n",
    "\n",
    "```bash\n",
    "python data/download_titanic.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac66f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and dataset load\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "DATA_PATH = Path('data') / 'titanic.csv'\n",
    "if not DATA_PATH.exists():\n",
    "    print('Dataset not found locally, downloading...')\n",
    "    import runpy\n",
    "    runpy.run_path('data/download_titanic.py')\n",
    "\n",
    "print('Loading', DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8925986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "print('shape:', df.shape)\n",
    "print('\\nSurvived value counts:')\n",
    "print(df['Survived'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Survived', data=df)\n",
    "plt.title('Survived (0 = Died, 1 = Survived)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(df['Age'].dropna(), kde=True)\n",
    "plt.title('Age distribution')\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(x='Survived', y='Fare', data=df)\n",
    "plt.title('Fare by Survival')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df2 = df.copy()\n",
    "# Title extraction\n",
    "df2['Title'] = df2['Name'].str.extract(',\\s*([^\\.]+)\\.', expand=False).str.strip()\n",
    "# group rare titles\n",
    "title_map = {\n",
    "    'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n",
    "    'Lady': 'Rare', 'Countess': 'Rare', 'Capt': 'Rare', 'Col': 'Rare', 'Don': 'Rare',\n",
    "    'Dr': 'Rare', 'Major': 'Rare', 'Rev': 'Rare', 'Sir': 'Rare', 'Jonkheer': 'Rare'\n",
    "}\n",
    "df2['Title'] = df2['Title'].replace(title_map)\n",
    "\n",
    "# Family size and is alone\n",
    "df2['FamilySize'] = df2['SibSp'] + df2['Parch'] + 1\n",
    "df2['IsAlone'] = (df2['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Fill missing embark/age/fare\n",
    "df2['Embarked'] = df2['Embarked'].fillna(df2['Embarked'].mode()[0])\n",
    "# Age: fill by median per Title, fallback to overall median\n",
    "age_medians = df2.groupby('Title')['Age'].median()\n",
    "def fill_age(row):\n",
    "    if pd.isna(row['Age']):\n",
    "        t = row['Title']\n",
    "        if not pd.isna(age_medians.get(t)):\n",
    "            return age_medians.get(t)\n",
    "        return df2['Age'].median()\n",
    "    return row['Age']\n",
    "\n",
    "df2['Age'] = df2.apply(fill_age, axis=1)\n",
    "\n",
    "# Fare: small fill\n",
    "df2['Fare'] = df2['Fare'].fillna(df2['Fare'].median())\n",
    "\n",
    "# Create Fare band\n",
    "df2['FareBand'] = pd.qcut(df2['Fare'], 4, labels=False)\n",
    "\n",
    "# Keep a compact set of features for modeling\n",
    "FEATURES = ['Pclass','Sex','Age','Fare','Embarked','Title','FamilySize','IsAlone']\n",
    "TARGET = 'Survived'\n",
    "df_model = df2[FEATURES + [TARGET]].copy()\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and modeling pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df_model.drop(columns=TARGET)\n",
    "y = df_model[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "numeric_features = ['Age', 'Fare', 'FamilySize']\n",
    "categorical_features = ['Pclass','Sex','Embarked','Title','IsAlone']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor), ('clf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_res = cross_validate(pipe, X_train, y_train, cv=cv, scoring=['accuracy','precision','recall'], n_jobs=1)\n",
    "\n",
    "print('CV accuracy: {:.3f} ± {:.3f}'.format(cv_res['test_accuracy'].mean(), cv_res['test_accuracy'].std()))\n",
    "print('CV precision: {:.3f} ± {:.3f}'.format(cv_res['test_precision'].mean(), cv_res['test_precision'].std()))\n",
    "print('CV recall: {:.3f} ± {:.3f}'.format(cv_res['test_recall'].mean(), cv_res['test_recall'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'clf__n_estimators': [100, 200, 300, 500],\n",
    "    'clf__max_depth': [None, 5, 8, 15, 25],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(pipe, param_dist, n_iter=25, cv=cv, scoring='accuracy', n_jobs=-1, random_state=42, verbose=1)\n",
    "rs.fit(X_train, y_train)\n",
    "print('Best score:', rs.best_score_)\n",
    "print('Best params:', rs.best_params_)\n",
    "\n",
    "best = rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on hold-out test set\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "y_pred = best.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "if hasattr(best, 'predict_proba'):\n",
    "    y_proba = best.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print('ROC AUC:', auc)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC={auc:.3f}')\n",
    "    plt.plot([0,1],[0,1],'--',color='gray')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend(); plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4,3)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues'); plt.title('Confusion matrix'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (permutation importance)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Build feature names (numeric + onehot categories)\n",
    "pre = best.named_steps['preprocessor']\n",
    "cat_ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "onehot_names = []\n",
    "for name, cats in zip(['Pclass','Sex','Embarked','Title','IsAlone'], cat_ohe.categories_):\n",
    "    onehot_names += [f'{name}_{c}' for c in cats]\n",
    "feature_names = ['Age','Fare','FamilySize'] + onehot_names\n",
    "\n",
    "X_test_trans = pre.transform(X_test)\n",
    "res = permutation_importance(best.named_steps['clf'], X_test_trans, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "imp_df = pd.DataFrame({'feature': feature_names, 'importance': res.importances_mean}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(8,5)); sns.barplot(data=imp_df, x='importance', y='feature', palette='viridis'); plt.title('Permutation importance'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explainability (TreeExplainer for tree-based model)\n",
    "import shap\n",
    "\n",
    "# Use a small sample for plotting speed\n",
    "X_sample = X_train.sample(200, random_state=42)\n",
    "X_sample_trans = pre.transform(X_sample)\n",
    "explainer = shap.TreeExplainer(best.named_steps['clf'])\n",
    "shap_values = explainer.shap_values(X_sample_trans)\n",
    "\n",
    "# For binary classification shap_values is a list; show for class 1\n",
    "shap.summary_plot(shap_values[1], X_sample_trans, feature_names=feature_names, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME explanation example\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# LIME expects raw training data (numpy) and a predict_proba function\n",
    "X_train_for_lime = X_train.copy()\n",
    "explainer = LimeTabularExplainer(X_train_for_lime.values, feature_names=X_train_for_lime.columns.tolist(), class_names=['died','survived'], discretize_continuous=True, random_state=42)\n",
    "\n",
    "instance = X_test.iloc[7]\n",
    "exp = explainer.explain_instance(instance.values, best.predict_proba, num_features=6)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bc732",
   "metadata": {},
   "source": [
    "## Summary & next steps\n",
    "\n",
    "- We replaced the synthetic demo with the actual Titanic dataset and added realistic EDA, feature engineering, cross-validation, and hyperparameter tuning.\n",
    "- We used SHAP and LIME to provide model explanations (global and local).\n",
    "\n",
    "Next steps (optional): try a gradient-boosted model (XGBoost / LightGBM), add calibration, and push a Binder/Colab-friendly environment file (`environment.yml`) to ensure reproducible hosted runs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
